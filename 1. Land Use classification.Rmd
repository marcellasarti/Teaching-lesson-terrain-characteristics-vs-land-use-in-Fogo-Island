---
title: "Terrain characteristics vs Land use in Fogo Island"
author: "Marcella Sarti Arellano"
date: "2024-09-10"
output:
  html_document: default
  word_document: default
---

# Research question
1.a)
Does terrain characteristics (eg. exposition, slope) correlate with land cover, ie. are there major differences between single land covers?

# Libraries
```{r}
library(terra)
library(raster)
library(randomForest)
library(caret)
library(data.table)
library(sf)
library(dplyr)
library(vcd)
library(FactoMineR)
library(factoextra)
```

# 1. Creating a Stack
Sentinel raster with bands 2 (blue), 3 (green), 4 (red) & 8 (NIR).
```{r}
rast1 <- rast("WVP_10m.jp2")
rast2 <- rast("AOT_10m.jp2")
rast3 <-  rast("B02_10m.jp2")
rast4 <-  rast("B03_10m.jp2")
rast5 <-  rast("B04_10m.jp2")
rast6 <-  rast("B08_10m.jp2")
rast7 <-  rast("TCI_10m.jp2")

rast_fogo <- c(rast3,rast4,rast5,rast6) 
plot(rast_fogo)
```

Crop & Stack

```{r}
# extension

crop_extent <- ext(c(765000,795000,1638000,1667000)) #xmin, xmax, ymin, ymax

# crop stack

fogo_crop <- crop(rast_fogo, crop_extent)
```

```{r}
plot(fogo_crop$B08_10m)
```

```{r}
writeRaster(fogo_crop, "fogo_crop.tif", overwrite=TRUE)
```

# 2. NDVI

True color composition
```{r}
# true
plotRGB(fogo_crop, b =1,
        g =2,
        r = 3,
        stretch="lin")
```


```{r}
# false

plotRGB(fogo_crop, b =1,
        g =2,
        r = 4, # NRI
        stretch="lin")
```
Calculate NDVI
ndvi = (NIR - RED)/(NIR + RED)

```{r}
ndvi = (fogo_crop[[4]] - fogo_crop[[3]]) / (fogo_crop[[4]] + fogo_crop[[3]])

plot(ndvi)

```

Other method: function for NDVI

```{r}
NDVI <- function(nir, red){
  if (length(nir) != length(red)){
    stop("NIR and RED dont match")
  }
  
  ndvi <- (nir-red) / (nir+red)
  names(ndvi)<- "ndvi"
  
  ndvi
}

```

 
```{r}
# Apply the function

ndvi_fog <-NDVI(fogo_crop$B08_10m, fogo_crop$B04_10m)

plot(ndvi_fog)
```




Unir NDVI con el stack
```{r}
result <- c(fogo_crop, ndvi)

names(result) <- c("B02_may", "B03_may","B04_may", "B08_may","NDVI_may")

```


```{r}
writeRaster(result, "fogo_ndvi2.tif", overwrite=TRUE, datatype = "FLT4S" )
```

# 3. Classification and Regresion with Random forest

```{r}
sentinel_nov <- rast("sentinel2_20221127_ndvi.tif")
sentinel_may <- rast("fogo_ndvi2.tif")
```

```{r}
# combine
sentinel <- c(sentinel_may, sentinel_nov)
```


```{r}
# polygons

ref_data <- vect("reference1.gpkg")

table(ref_data$class)
```

we verify if it's the same place in may and nov

```{r}
# may
plotRGB(sentinel, b =1,
        g =2,
        r = 3,
        stretch="lin")
```


```{r}
# nov
plotRGB(sentinel, b =6,
        g =7,
        r = 8,
        stretch="lin")

plot(ref_data, add=TRUE, col ="red")
```


Random forest

```{r}
df_extract <- extract(sentinel, ref_data, na.rm = TRUE)
ref_data$ID <- seq(1, nrow( ref_data))
df_extract <- merge(df_extract, ref_data)


# create model 

rfmodel = randomForest(x = df_extract[,c("B02_may", "B03_may", "B04_may", "B08_may", "NDVI_may",
                                 "B02_nov", "B03_nov", "B04_nov", "B08_nov", "NDVI_nov")],
                       y = as.factor( df_extract[, c("class")] ), ntree = 100)

```



Criteria for classification:

* Agriculture: green pixels, red color in NRI, plots in Google satellite 
*   Basaltic rock (old): combination of black, gray and dark green colors, in Google satellite is brown with a pronounce texture 
*   Basaltic rock (young): most dark pixels patches
*   Forest: large trees in a big patch, red color in NRI, deep green in Google satellite
*   Human settlements: infrastructure in Google satellite
*  Land with little or no vegetation: mostly uniform beige pixels with some green
*   Ocean: different shades of blue pixels colors

```{r}
# apply model

lcc <- predict(sentinel, rfmodel)
plot(lcc, col=c("orange", "brown", "black", "darkgreen","purple","yellowgreen","darkblue"))
```

# 4. Accuracy of the model

```{r}
# Create a partition (70% training data, 30% test data)
trainIndex <- createDataPartition(df_extract$class, p = 0.7, list = FALSE)

# Split data into training and testing sets
trainData <- df_extract[trainIndex, ] #data que si esta viendo el modelo

testData  <- df_extract[-trainIndex, ] #data que no esta viendo el modelo
```

```{r}
rfmodel2 = randomForest(x = trainData[,c("B02_may", "B03_may", "B04_may", "B08_may", "NDVI_may",
                                           "B02_nov", "B03_nov", "B04_nov", "B08_nov", "NDVI_nov")],
                       y = as.factor( trainData[, c("class")] ),
                       ntree = 100)
```

Apply model with traindata and testdata
```{r}
lcc2 = predict(sentinel, rfmodel2)
plot(lcc2, col=c("orange", "brown", "black", "darkgreen","purple","yellowgreen","darkblue"))
```
```{r}
writeRaster(lcc2, "fogo_model2.tif", overwrite = TRUE)
```


Accuracy with test data

```{r}
# predict the classes of the independent test set
test_prediction = predict(rfmodel2, testData)

# confusion matrix
cfm = table(testData$class, test_prediction)
cfm
# accuracy: sum of diagonals divided by all
sum(diag(cfm))/sum(cfm)

```


Cross validation

```{r}
# set up cross validation with 5 folds 
trc = trainControl(method = "cv", number = 5)


# tune model mtry
rfmodel_cv_may = caret::train(x = trainData[,c("B02_may", "B03_may", "B04_may", "B08_may", "NDVI_may")],
                          y = trainData[, c("class")],
                          method = "rf",
                          trControl = trc,
                          ntree = 100,
                          tuneLength = 3) 

rfmodel_cv_nov = caret::train(x = trainData[,c("B02_nov", "B03_nov", "B04_nov", "B08_nov", "NDVI_nov")],
                          y = trainData[, c("class")],
                          method = "rf",
                          trControl = trc,
                          ntree = 100,
                          tuneLength = 3)

rfmodel_cv_full = caret::train(x = trainData[,c("B02_may", "B03_may", "B04_may", "B08_may", "NDVI_may",
                                              "B02_nov", "B03_nov", "B04_nov", "B08_nov", "NDVI_nov")],
                          y = trainData[, c("class")],
                          method = "rf",
                          trControl = trc,
                          ntree = 100,
                          tuneLength = 3)

```

```{r}
# predict the classes of the independet test set

test_prediction <- predict(rfmodel_cv_full, testData)

# confusion matrix

confusionMatrix(as.factor(testData$class), test_prediction, mode="everything")
```
* Accuracy:

Definition: The proportion of correctly classified instances among the total instances.
Value: 0.9531 (95.31%)
Interpretation: The model correctly classified 95.31% of all instances.

* 95% CI (Confidence Interval):

Definition: The range within which the true accuracy is expected to fall, with 95% confidence.
Value: (0.9511, 0.9551)
Interpretation: We are 95% confident that the true accuracy of the model lies between 95.11% and 95.51%.


* P-Value [Acc > NIR]:

Definition: The p-value testing whether the model's accuracy is significantly better than the No Information Rate (NIR), which is the accuracy expected by chance.
Value: < 2.2e-16
Interpretation: The model's accuracy is highly significantly better than random chance, with a p-value much smaller than 0.05.

* Kappa:

Definition: A measure of the agreement between predicted and actual classifications, adjusting for chance agreement.
Value: 0.9364
Interpretation: There is a very high level of agreement between the predicted and actual classes, much better than random chance.

* Precision (Positive Predictive Value)~User's accuracy:

Definition: The proportion of correctly predicted positive instances among all predicted positives.
Interpretation: Precision is calculated per class, for example:
For "bare_ground": 0.9117 (91.17%) means that when the model predicts "bare_ground," it is correct 91.17% of the time.
Precision values vary by class, reflecting the model's reliability for each class prediction.


* Recall (Sensitivity)~Procuder's accuracy:

Definition: The proportion of correctly predicted positives among all actual positives.
Interpretation: Recall is also calculated per class:
For "bare_ground": 0.8734 (87.34%) means that the model correctly identifies 87.34% of all actual "bare_ground" instances.
Higher recall means fewer actual positives are missed.

* F1 Score:

Definition: The harmonic mean of precision and recall, providing a balanced measure that accounts for both false positives and false negatives.
Interpretation: A high F1 score indicates a good balance between precision and recall.
